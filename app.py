# ============================================================
#  ë„¤ì´ë²„ ë‰´ìŠ¤ í´ë¦¬í•‘ v4 - Streamlit ì•±
#
#  ì‹¤í–‰ ë°©ë²•:
#    pip install streamlit requests beautifulsoup4 pandas xlsxwriter
#    streamlit run naver_news_clipping_streamlit.py
# ============================================================

import io
import re
import html
import time
import requests
import pandas as pd
import streamlit as st
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from bs4 import BeautifulSoup

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ì„¤ì •ê°’
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MAX_WORKERS     = 10
REQUEST_TIMEOUT = 6
HEADERS = {
    'User-Agent': (
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
        'AppleWebKit/537.36 (KHTML, like Gecko) '
        'Chrome/124.0.0.0 Safari/537.36'
    )
}

GROUP_COLORS = {
    "ê·¸ë£¹ A": "#D5F5E3",
    "ê·¸ë£¹ B": "#FEF9E7",
    "ê·¸ë£¹ C": "#FDEBD0",
    "":       "#FFFFFF",
}

# â”€â”€ ê·¸ë£¹ ë°°ì§€ ìƒ‰ìƒ (Streamlit í…Œì´ë¸”ìš© HTML) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GROUP_BADGE = {
    "ê·¸ë£¹ A": "background:#D5F5E3; color:#1e7e34; padding:2px 8px; border-radius:4px; font-weight:bold;",
    "ê·¸ë£¹ B": "background:#FEF9E7; color:#856404; padding:2px 8px; border-radius:4px; font-weight:bold;",
    "ê·¸ë£¹ C": "background:#FDEBD0; color:#c05621; padding:2px 8px; border-radius:4px; font-weight:bold;",
    "":       "color:#999; padding:2px 8px;",
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ë§¤í•‘ í…Œì´ë¸”
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

FIXED_MAP = {
    "1conomynews": "1ì½”ë…¸ë¯¸ë‰´ìŠ¤",
    "cctimes": "ì¶©ì²­íƒ€ì„ì¦ˆ",
    "chungnamilbo": "ì¶©ë‚¨ì¼ë³´",
    "dtnews24": "ëŒ€ì „ë‰´ìŠ¤",
    "enetnews": "ì´ë„·ë‰´ìŠ¤",
    "financialreview": "íŒŒì´ë‚¸ì…œë¦¬ë·°",
    "globalepic": "ê¸€ë¡œë²Œì—í”½",
    "gokorea": "ê³ ì½”ë¦¬ì•„",
    "goodmorningcc": "êµ¿ëª¨ë‹ì¶©ì²­",
    "hinews": "í•˜ì´ë‰´ìŠ¤",
    "idaegu": "ì•„ì´ëŒ€êµ¬",
    "joongdo": "ì¤‘ë„ì¼ë³´",
    "kdfnews": "í•œêµ­ë©´ì„¸ë‰´ìŠ¤",
    "ktnews": "ê°•ì›íƒ€ì„ì¦ˆ",
    "newslock": "ë‰´ìŠ¤ë½",
    "newsway": "ë‰´ìŠ¤ì›¨ì´",
    "opinionnews": "ì˜¤í”¼ë‹ˆì–¸ë‰´ìŠ¤",
    "startuptoday": "ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´",
    "straightnews": "ìŠ¤íŠ¸ë ˆì´íŠ¸ë‰´ìŠ¤",
    "tfmedia": "ì¡°ì„¸ê¸ˆìœµì‹ ë¬¸",
    "weekly": "CNBì €ë„",
    "wolyo": "ì›”ìš”ì‹ ë¬¸",
    "womaneconomy": "ì—¬ì„±ê²½ì œì‹ ë¬¸",
    "lawissue": "ë¡œì´ìŠˆ", "newsworker": "ë‰´ìŠ¤ì›Œì»¤", "topdaily": "í†±ë°ì¼ë¦¬",
    "wikitree": "ìœ„í‚¤íŠ¸ë¦¬", "thepublic": "ë”í¼ë¸”ë¦­", "thebigdata": "ë¹…ë°ì´í„°ë‰´ìŠ¤",
    "socialvalue": "ì†Œì…œë°¸ë¥˜", "smartfn": "ìŠ¤ë§ˆíŠ¸ì—í”„ì—”", "sisacast": "ì‹œì‚¬ìºìŠ¤íŠ¸",
    "siminilbo": "ì‹œë¯¼ì¼ë³´", "seoultimes": "ì„œìš¸íƒ€ì„ì¦ˆ", "sentv": "ì„œìš¸ê²½ì œTV",
    "segyebiz": "ì„¸ê³„ë¹„ì¦ˆ", "pressman": "í”„ë ˆìŠ¤ë§¨", "popcornnews": "íŒì½˜ë‰´ìŠ¤",
    "pointe": "í¬ì¸íŠ¸ë°ì¼ë¦¬", "onews": "ì—´ë¦°ë‰´ìŠ¤í†µì‹ ", "nextdaily": "ë„¥ìŠ¤íŠ¸ë°ì¼ë¦¬",
    "newswatch": "ë‰´ìŠ¤ì›Œì¹˜", "newsquest": "ë‰´ìŠ¤í€˜ìŠ¤íŠ¸", "newsprime": "ë‰´ìŠ¤í”„ë¼ì„",
    "newsinside": "ë‰´ìŠ¤ì¸ì‚¬ì´ë“œ", "mkhealth": "ë§¤ê²½í—¬ìŠ¤", "metroseoul": "ë©”íŠ¸ë¡œì‹ ë¬¸",
    "meconomynews": "Mì´ì½”ë…¸ë¯¸", "kbsm": "ê²½ë¶ì‹ ë¬¸", "joongangenews": "ì¤‘ì•™ì´ì½”ë…¸ë¯¸ë‰´ìŠ¤",
    "iminju": "ë¯¼ì£¼ì‹ ë¬¸", "ilyo": "ì¼ìš”ì‹ ë¬¸", "hankooki": "ìŠ¤í¬ì¸ í•œêµ­",
    "ezyeconomy": "ì´ì§€ê²½ì œ", "enewstoday": "ì´ë‰´ìŠ¤íˆ¬ë°ì´", "ekn": "ì—ë„ˆì§€ê²½ì œ",
    "dizzotv": "ë””ì§€í‹€ì¡°ì„ ì¼ë³´", "cstimes": "ì»¨ìŠˆë¨¸íƒ€ì„ìŠ¤",
    "consumernews": "ì†Œë¹„ìê°€ë§Œë“œëŠ”ì‹ ë¬¸", "ceoscoredaily": "CEOìŠ¤ì½”ì–´ë°ì¼ë¦¬",
    "breaknews": "ë¸Œë ˆì´í¬ë‰´ìŠ¤", "bizwnews": "ë¹„ì¦ˆì›”ë“œ", "beyondpost": "ë¹„ìš˜ë“œí¬ìŠ¤íŠ¸",
    "asiatime": "ì•„ì‹œì•„íƒ€ì„ì¦ˆ", "apnews": "ì•„ì‹œì•„ì—ì´", "biz": "ë‰´ë°ì¼ë¦¬",
    "viva100": "ë¸Œë¦¿ì§€ê²½ì œ", "srtimes": "SRíƒ€ì„ìŠ¤", "kpenews": "í•œêµ­ì •ê²½ì‹ ë¬¸",
    "news2day": "ë‰´ìŠ¤íˆ¬ë°ì´", "fashionbiz": "íŒ¨ì…˜ë¹„ì¦ˆ", "econovill": "ì´ì½”ë…¸ë¯¹ë¦¬ë·°",
    "businessplus": "ë¹„ì¦ˆë‹ˆìŠ¤í”ŒëŸ¬ìŠ¤", "newspim": "ë‰´ìŠ¤í•Œ", "m-i": "ë§¤ì¼ì¼ë³´",
    "pointdaily": "í¬ì¸íŠ¸ë°ì¼ë¦¬", "ajunews": "ì•„ì£¼ê²½ì œ", "asiatoday": "ì•„ì‹œì•„íˆ¬ë°ì´", "xportsnews": "ì—‘ìŠ¤í¬ì¸ ë‰´ìŠ¤", "sports": "ì—‘ìŠ¤í¬ì¸ ë‰´ìŠ¤", "youthdaily": "ì²­ë…„ì¼ë³´",
    "seoulwire": "ì„œìš¸ì™€ì´ì–´", "newstomato": "ë‰´ìŠ¤í† ë§ˆí† ", "widedaily": "ì™€ì´ë“œê²½ì œ",
    "apparelnews": "ì–´íŒ¨ëŸ´ë‰´ìŠ¤", "biztribune": "ë¹„ì¦ˆíŠ¸ë¦¬ë·´", "etoday": "ì´íˆ¬ë°ì´",
    "ngetnews": "ë‰´ìŠ¤ì €ë„ë¦¬ì¦˜", "hansbiz": "í•œìŠ¤ê²½ì œ", "byline": "ë°”ì´ë¼ì¸ë„¤íŠ¸ì›Œí¬",
    "dealsite": "ë”œì‚¬ì´íŠ¸", "businesspost": "ë¹„ì¦ˆë‹ˆìŠ¤í¬ìŠ¤íŠ¸", "dnews": "ëŒ€í•œê²½ì œ",
    "insight": "ì¸ì‚¬ì´íŠ¸", "slist": "ì‹±ê¸€ë¦¬ìŠ¤íŠ¸", "theviewers": "ë·°ì–´ìŠ¤",
    "daily": "ë°ì¼ë¦¬í•œêµ­", "veritas-a": "ë² ë¦¬íƒ€ìŠ¤ì•ŒíŒŒ", "fortunekorea": "í¬ì¶˜ì½”ë¦¬ì•„",
    "huffingtonpost": "í—ˆí•‘í„´í¬ìŠ¤íŠ¸", "mediapen": "ë¯¸ë””ì–´íœ", "paxetv": "íŒìŠ¤ê²½ì œTV",
    "shinailbo": "ì‹ ì•„ì¼ë³´", "pinpointnews": "í•€í¬ì¸íŠ¸ë‰´ìŠ¤", "sisunnews": "ì‹œì„ ë‰´ìŠ¤",
    "sisaon": "ì‹œì‚¬ì˜¨", "smarttoday": "ìŠ¤ë§ˆíŠ¸íˆ¬ë°ì´", "ziksir": "ì§ì°",
    "job-post": "ì¡í¬ìŠ¤íŠ¸", "issuenbiz": "ì´ìŠˆì•¤ë¹„ì¦ˆ", "fashionn": "íŒ¨ì…˜ì—”",
    "econonews": "ì´ì½”ë…¸ë‰´ìŠ¤",
}

OID_MAP = {
    "001": "ì—°í•©ë‰´ìŠ¤", "002": "í”„ë ˆì‹œì•ˆ", "003": "ë‰´ì‹œìŠ¤", "004": "ë‚´ì¼ì‹ ë¬¸",
    "005": "êµ­ë¯¼ì¼ë³´", "008": "ë¨¸ë‹ˆíˆ¬ë°ì´", "009": "ë§¤ì¼ê²½ì œ", "011": "ì„œìš¸ê²½ì œ",
    "014": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤", "015": "í•œêµ­ê²½ì œ", "016": "í—¤ëŸ´ë“œê²½ì œ", "018": "ì´ë°ì¼ë¦¬",
    "020": "ë™ì•„ì¼ë³´", "021": "ë¬¸í™”ì¼ë³´", "022": "ì„¸ê³„ì¼ë³´", "023": "ì¡°ì„ ì¼ë³´",
    "025": "ì¤‘ì•™ì¼ë³´", "028": "í•œê²¨ë ˆ", "029": "ë””ì§€í„¸íƒ€ì„ìŠ¤", "030": "ì „ìì‹ ë¬¸",
    "031": "ì•„ì´ë‰´ìŠ¤24", "032": "ê²½í–¥ì‹ ë¬¸", "034": "ì´ì½”ë…¸ë¯¸ìŠ¤íŠ¸", "038": "í•œêµ­ì¼ë³´",
    "052": "YTN", "055": "SBS", "056": "KBS", "057": "MBN", "065": "ìŠ¤í¬ì¸ ì„œìš¸",
    "076": "ìŠ¤í¬ì¸ ì¡°ì„ ", "079": "ë…¸ì»·ë‰´ìŠ¤", "081": "ì„œìš¸ì‹ ë¬¸", "082": "ë¶€ì‚°ì¼ë³´",
    "088": "ë§¤ì¼ì‹ ë¬¸", "092": "ì§€ë””ë„·ì½”ë¦¬ì•„", "117": "ë§ˆì´ë°ì¼ë¦¬", "119": "ë°ì¼ë¦¬ì•ˆ",
    "123": "ì¡°ì„¸ì¼ë³´", "138": "ë””ì§€í„¸ë°ì¼ë¦¬", "143": "ì¿ í‚¤ë‰´ìŠ¤", "144": "ìŠ¤í¬ì¸ ì›”ë“œ",
    "214": "MBC", "215": "í•œêµ­ê²½ì œTV", "241": "ì‹œì‚¬IN", "243": "ì´ì½”ë…¸ë¯¸ìŠ¤íŠ¸",
    "277": "ì•„ì‹œì•„ê²½ì œ", "584": "ì•„ì‹œì•„íˆ¬ë°ì´", "293": "ë¸”ë¡œí„°", "321": "ë¸Œë¦¿ì§€ê²½ì œ", "323": "í•œêµ­ì„¬ìœ ì‹ ë¬¸",
    "324": "ì´íˆ¬ë°ì´", "329": "ë‰´ë°ì¼ë¦¬", "366": "ì¡°ì„ ë¹„ì¦ˆ", "374": "SBS Biz",
    "383": "í•œêµ­ì •ê²½ì‹ ë¬¸", "410": "ì–´íŒ¨ëŸ´ë‰´ìŠ¤", "417": "ë¨¸ë‹ˆS", "421": "ë‰´ìŠ¤1",
    "437": "JTBC", "445": "ëŒ€í•œê²½ì œ", "448": "ì„œìš¸ì™€ì´ì–´", "449": "TVì¡°ì„ ",
    "465": "ì—¬ì„±ê²½ì œì‹ ë¬¸", "468": "ìŠ¤í¬ì¸ ê²½í–¥", "512": "ë‰´ìŠ¤í•Œ", "529": "ì‹±ê¸€ë¦¬ìŠ¤íŠ¸",
    "586": "ì‹œì‚¬ì €ë„e", "629": "ë‰´ìŠ¤í† ë§ˆí† ", "645": "ì•„ì£¼ê²½ì œ", "648": "ë¹„ì¦ˆì›Œì¹˜",
    "654": "ë¹„ì¦ˆíŠ¸ë¦¬ë·´", "658": "ë·°ì–´ìŠ¤", "660": "ì²­ë…„ì¼ë³´", "929": "ë””ì§€í„¸íˆ¬ë°ì´",
    "239": "ë°”ì´ë¼ì¸ë„¤íŠ¸ì›Œí¬", "273": "íŒ¨ì…˜ë¹„ì¦ˆ",
}

GROUP_MAP = {
    "1ì½”ë…¸ë¯¸ë‰´ìŠ¤":"ê·¸ë£¹ B","CBSë…¸ì»·ë‰´ìŠ¤":"ê·¸ë£¹ A","CEO ìŠ¤ì½”ì–´ë°ì¼ë¦¬":"ê·¸ë£¹ C",
    "EBN":"ê·¸ë£¹ B","FETV":"ê·¸ë£¹ C","ITì¡°ì„ ":"ê·¸ë£¹ C","KBS":"ê·¸ë£¹ A",
    "KíŒ¨ì…˜ë‰´ìŠ¤":"ê·¸ë£¹ C","MBC":"ê·¸ë£¹ A","MBN":"ê·¸ë£¹ A","S-ì €ë„":"ê·¸ë£¹ C",
    "SBS":"ê·¸ë£¹ A","SBS Biz":"ê·¸ë£¹ A","SRíƒ€ì„ìŠ¤":"ê·¸ë£¹ C","TVì¡°ì„ ":"ê·¸ë£¹ A",
    "YTN":"ê·¸ë£¹ A","ê²½í–¥ì‹ ë¬¸":"ê·¸ë£¹ A","ê³µê³µë‰´ìŠ¤":"ê·¸ë£¹ B","êµ­ë¯¼ì¼ë³´":"ê·¸ë£¹ A",
    "êµ­ì œì„¬ìœ ì‹ ë¬¸":"ê·¸ë£¹ A","êµ¿ëª¨ë‹ê²½ì œ":"ê·¸ë£¹ C","ë‚¨ë‹¤ë¥¸ë””í…Œì¼":"ê·¸ë£¹ B",
    "ë‚´ì¼ì‹ ë¬¸":"ê·¸ë£¹ A","ë…¹ìƒ‰ê²½ì œì‹ ë¬¸":"ê·¸ë£¹ C","ë‰´ë°ì¼ë¦¬":"ê·¸ë£¹ A","ë‰´ìŠ¤1":"ê·¸ë£¹ A",
    "ë‰´ìŠ¤ì›Œì¹˜":"ê·¸ë£¹ C","ë‰´ìŠ¤ì›Œì»¤":"ê·¸ë£¹ C","ë‰´ìŠ¤ì›¨ì´":"ê·¸ë£¹ B","ë‰´ìŠ¤ì¸ì‚¬ì´ë“œ":"ê·¸ë£¹ C",
    "ë‰´ìŠ¤ì €ë„ë¦¬ì¦˜":"ê·¸ë£¹ B","ë‰´ìŠ¤í† ë§ˆí† ":"ê·¸ë£¹ C","ë‰´ìŠ¤í†±":"ê·¸ë£¹ B","ë‰´ìŠ¤íˆ¬ë°ì´":"ê·¸ë£¹ B",
    "ë‰´ìŠ¤í¬ìŠ¤íŠ¸":"ê·¸ë£¹ C","ë‰´ìŠ¤í•Œ":"ê·¸ë£¹ A","ë‰´ì‹œìŠ¤":"ê·¸ë£¹ A","ë‰´ì‹œì•ˆ":"ê·¸ë£¹ C",
    "ëŒ€í•œê²½ì œ":"ê·¸ë£¹ B","ë”ë¦¬ë¸ŒìŠ¤":"ê·¸ë£¹ C","ë”ë°¸ë¥˜ë‰´ìŠ¤":"ê·¸ë£¹ B","ë”ë²¨":"ê·¸ë£¹ B",
    "ë”ìŠ¤ì¿ í”„":"ê·¸ë£¹ B","ë”ìŠ¤íƒ":"ê·¸ë£¹ B","ë”íŒ©íŠ¸":"ê·¸ë£¹ A","ë”í”¼ì•Œ":"ê·¸ë£¹ C",
    "ë°ì¼ë¦¬ì•ˆ":"ê·¸ë£¹ A","ë°ì¼ë¦¬í•œêµ­":"ê·¸ë£¹ A","ë™ì•„ë‹·ì»´":"ê·¸ë£¹ C","ë™ì•„ì¼ë³´":"ê·¸ë£¹ A",
    "ë™í–‰ë¯¸ë””ì–´ ì‹œëŒ€":"ê·¸ë£¹ A","ë””ì§€í„¸ë°ì¼ë¦¬":"ê·¸ë£¹ A","ë””ì§€í„¸íƒ€ì„ìŠ¤":"ê·¸ë£¹ A",
    "ë””ì§€í„¸íˆ¬ë°ì´":"ê·¸ë£¹ B","ë””ì§€í‹€ì¡°ì„ ì¼ë³´":"ê·¸ë£¹ C","ë””í† ì•¤ë””í† ":"ê·¸ë£¹ A",
    "ë”œì‚¬ì´íŠ¸":"ê·¸ë£¹ B","ë”œì‚¬ì´íŠ¸TV":"ê·¸ë£¹ C","ë¡œì´ìŠˆ":"ê·¸ë£¹ B","ë§ˆì´ë°ì¼ë¦¬":"ê·¸ë£¹ B",
    "ë§¤ê²½ì´ì½”ë…¸ë¯¸":"ê·¸ë£¹ B","ë§¤ê²½í—¬ìŠ¤":"ê·¸ë£¹ B","ë§¤ì¼ê²½ì œ":"ê·¸ë£¹ A",
    "ë§¤ì¼ê²½ì œ ë ˆì´ë”M":"ê·¸ë£¹ B","ë§¤ì¼ê²½ì œTV":"ê·¸ë£¹ C","ë§¤ì¼ì‹ ë¬¸":"ê·¸ë£¹ B",
    "ë§¤ì¼ì¼ë³´":"ê·¸ë£¹ B","ë¨¸ë‹ˆíˆ¬ë°ì´":"ê·¸ë£¹ A","ë¨¸ë‹ˆíˆ¬ë°ì´ë°©ì†¡":"ê·¸ë£¹ A",
    "ë©”ê°€ê²½ì œ":"ê·¸ë£¹ C","ë©”íŠ¸ë¡œì‹ ë¬¸":"ê·¸ë£¹ C","ë¬¸í™”ì¼ë³´":"ê·¸ë£¹ A","ë¬¸í™”ì €ë„21":"ê·¸ë£¹ C",
    "ë¯¸ë””ì–´íœ":"ê·¸ë£¹ C","ë°”ì´ë¼ì¸ë„¤íŠ¸ì›Œí¬":"ê·¸ë£¹ A","ë¶€ì‚°ì¼ë³´":"ê·¸ë£¹ B","ë·°ì–´ìŠ¤":"ê·¸ë£¹ C",
    "ë¸Œë¦¿ì§€ê²½ì œ":"ê·¸ë£¹ B","ë¸”ë¡œí„°":"ê·¸ë£¹ A","ë¹„ì¦ˆë‹ˆìŠ¤ì›Œì¹˜":"ê·¸ë£¹ A","ë¹„ì¦ˆë‹ˆìŠ¤í¬ìŠ¤íŠ¸":"ê·¸ë£¹ B",
    "ë¹„ì¦ˆë‹ˆìŠ¤í”ŒëŸ¬ìŠ¤":"ê·¸ë£¹ B","ë¹„ì¦ˆíŠ¸ë¦¬ë·´":"ê·¸ë£¹ C","ë¹„ì¦ˆí•œêµ­":"ê·¸ë£¹ C",
    "ì„œìš¸ê²½ì œ":"ê·¸ë£¹ A","ì„œìš¸ê²½ì œTV":"ê·¸ë£¹ A","ì„œìš¸ì‹ ë¬¸":"ê·¸ë£¹ A","ì„œìš¸ì™€ì´ì–´":"ê·¸ë£¹ C",
    "ì„œìš¸íŒŒì´ë‚¸ìŠ¤":"ê·¸ë£¹ C","ì„¸ê³„ë¹„ì¦ˆ":"ê·¸ë£¹ C","ì„¸ê³„ì¼ë³´":"ê·¸ë£¹ A",
    "ì†Œë¹„ìê°€ë§Œë“œëŠ”ì‹ ë¬¸":"ê·¸ë£¹ B","ì†Œì…œë°¸ë¥˜":"ê·¸ë£¹ C","ìŠ¤ë§ˆíŠ¸íˆ¬ë°ì´":"ê·¸ë£¹ C",
    "ìŠ¤íŠ¸ë ˆì´íŠ¸ë‰´ìŠ¤":"ê·¸ë£¹ C","ìŠ¤í¬ì¸ ì¡°ì„ ":"ê·¸ë£¹ B","ìŠ¤í¬ì¸ í•œêµ­":"ê·¸ë£¹ B",
    "ì‹œì‚¬ì˜¤ëŠ˜":"ê·¸ë£¹ C","ì‹œì‚¬ìœ„í¬":"ê·¸ë£¹ C","ì‹œì‚¬ì €ë„ì´ì½”ë…¸ë¯¸":"ê·¸ë£¹ C","ì‹œì‚¬ìºìŠ¤íŠ¸":"ê·¸ë£¹ C",
    "ì‹ ì•„ì¼ë³´":"ê·¸ë£¹ C","ì‹±ê¸€ë¦¬ìŠ¤íŠ¸":"ê·¸ë£¹ C","ì•„ì‹œì•„ê²½ì œ":"ê·¸ë£¹ A","ì•„ì‹œì•„íƒ€ì„ì¦ˆ":"ê·¸ë£¹ B",
    "ì•„ì‹œì•„íˆ¬ë°ì´":"ê·¸ë£¹ A","ì•„ì›ƒìŠ¤íƒ ë”©":"ê·¸ë£¹ A","ì•„ì´ë‰´ìŠ¤24":"ê·¸ë£¹ A","ì•„ì£¼ê²½ì œ":"ê·¸ë£¹ A",
    "ì•„ì£¼ì¼ë³´":"ê·¸ë£¹ C","ì•ŒíŒŒê²½ì œ":"ê·¸ë£¹ B","ì•½ì—…ì‹ ë¬¸":"ê·¸ë£¹ C","ì–´íŒ¨ëŸ´ë‰´ìŠ¤":"ê·¸ë£¹ A",
    "ì—ë„ˆì§€ê²½ì œ":"ê·¸ë£¹ B","ì—¬ì„±ê²½ì œì‹ ë¬¸":"ê·¸ë£¹ C","ì—°í•© ì¸í¬ë§¥ìŠ¤":"ê·¸ë£¹ B",
    "ì—°í•©ë‰´ìŠ¤":"ê·¸ë£¹ A","ì—°í•©ë‰´ìŠ¤TV":"ê·¸ë£¹ A","ì˜¤ëŠ˜ê²½ì œ":"ê·¸ë£¹ C","ì›”ìš”ì‹ ë¬¸":"ê·¸ë£¹ B",
    "ìœ„í‚¤ë¦¬í¬ìŠ¤í•œêµ­":"ê·¸ë£¹ B","ìœ„í‚¤íŠ¸ë¦¬":"ê·¸ë£¹ C","ì´ë‰´ìŠ¤íˆ¬ë°ì´":"ê·¸ë£¹ B","ì´ë°ì¼ë¦¬":"ê·¸ë£¹ A",
    "ì´ì½”ë…¸ë¯¸ìŠ¤íŠ¸":"ê·¸ë£¹ B","ì´ì½”ë…¸ë¯¹ë¦¬ë·°":"ê·¸ë£¹ B","ì´íˆ¬ë°ì´":"ê·¸ë£¹ A",
    "ì¸ë² ìŠ¤íŠ¸ì¡°ì„ ":"ê·¸ë£¹ B","ì¸ì‚¬ì´íŠ¸":"ê·¸ë£¹ C","ì¸ì‚¬ì´íŠ¸ì½”ë¦¬ì•„":"ê·¸ë£¹ B",
    "ì¼ê°„ìŠ¤í¬ì¸ ":"ê·¸ë£¹ B","ì¼ìš”ì„œìš¸":"ê·¸ë£¹ C","ì¼ìš”ì‹ ë¬¸":"ê·¸ë£¹ C","ì „ìì‹ ë¬¸":"ê·¸ë£¹ A",
    "ì¡°ì„ ë¹„ì¦ˆ":"ê·¸ë£¹ A","ì¡°ì„ ì¼ë³´":"ê·¸ë£¹ A","ì£¼ê°„í•œêµ­":"ê·¸ë£¹ B","ì¤‘ì†Œê¸°ì—…ì‹ ë¬¸":"ê·¸ë£¹ C",
    "ì¤‘ì•™ì„ ë°ì´":"ê·¸ë£¹ A","ì¤‘ì•™ì´ì½”ë…¸ë¯¸ë‰´ìŠ¤":"ê·¸ë£¹ C","ì¤‘ì•™ì¼ë³´":"ê·¸ë£¹ A",
    "ì§€ë””ë„·ì½”ë¦¬ì•„":"ê·¸ë£¹ A","ì²­ë…„ì¼ë³´":"ê·¸ë£¹ C","ì»¤ë„¥í„°ìŠ¤":"ê·¸ë£¹ C","ì»¨ìŠˆë¨¸íƒ€ì„ì¦ˆ":"ê·¸ë£¹ B",
    "ì½”ë¦¬ì•„ì¤‘ì•™ë°ì¼ë¦¬":"ê·¸ë£¹ A","ì½”ë¦¬ì•„íƒ€ì„ìŠ¤":"ê·¸ë£¹ A","ì½”ë¦¬ì•„í—¤ëŸ´ë“œ":"ê·¸ë£¹ A",
    "ì¿ í‚¤ë‰´ìŠ¤":"ê·¸ë£¹ A","í…Œë„ŒíŠ¸ë‰´ìŠ¤":"ê·¸ë£¹ A","í…Œí¬ì— ":"ê·¸ë£¹ A","í† ìš”ê²½ì œ":"ê·¸ë£¹ C",
    "í†±ë°ì¼ë¦¬":"ê·¸ë£¹ B","íˆ¬ë°ì´ì‹ ë¬¸":"ê·¸ë£¹ B","íˆ¬ë°ì´ì½”ë¦¬ì•„":"ê·¸ë£¹ C","íŒŒì´ë‚¸ì…œë‰´ìŠ¤":"ê·¸ë£¹ A",
    "íŒŒì´ë‚¸ì…œë¦¬ë·°":"ê·¸ë£¹ C","íŒŒì´ë‚¸ì…œíˆ¬ë°ì´":"ê·¸ë£¹ C","íŒŒì´ë‚¸ì…œí¬ìŠ¤íŠ¸":"ê·¸ë£¹ C",
    "íŒì½˜ë‰´ìŠ¤":"ê·¸ë£¹ C","íŒ¨ì…˜ë¹„ì¦ˆ":"ê·¸ë£¹ A","íŒ¨ì…˜ì¸ì‚¬ì´íŠ¸":"ê·¸ë£¹ A","íŒ¨ì…˜í¬ìŠ¤íŠ¸":"ê·¸ë£¹ A",
    "í¬ì¸íŠ¸ë°ì¼ë¦¬":"ê·¸ë£¹ C","í”„ë¼ì„ê²½ì œ":"ê·¸ë£¹ C","í•˜ì´ë‰´ìŠ¤":"ê·¸ë£¹ C","í•œê²¨ë ˆ":"ê·¸ë£¹ A",
    "í•œê²½ë¹„ì¦ˆë‹ˆìŠ¤":"ê·¸ë£¹ B","í•œêµ­ê²½ì œ":"ê·¸ë£¹ A","í•œêµ­ê²½ì œTV":"ê·¸ë£¹ A",
    "í•œêµ­ê¸ˆìœµì‹ ë¬¸":"ê·¸ë£¹ C","í•œêµ­ë©´ì„¸ë‰´ìŠ¤":"ê·¸ë£¹ C","í•œêµ­ì„¬ìœ ì‹ ë¬¸":"ê·¸ë£¹ A",
    "í•œêµ­ì¼ë³´":"ê·¸ë£¹ A","í•œêµ­ì •ê²½ì‹ ë¬¸":"ê·¸ë£¹ C","í•œìŠ¤ê²½ì œ":"ê·¸ë£¹ B","í—ˆí”„í¬ìŠ¤íŠ¸":"ê·¸ë£¹ C",
    "í—¤ëŸ´ë“œê²½ì œ":"ê·¸ë£¹ A","í˜„ëŒ€ê²½ì œì‹ ë¬¸":"ê·¸ë£¹ C","í›„ì§€TV":"ê·¸ë£¹ C","MTN":"ê·¸ë£¹ A",
}



# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  í—¬í¼ í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def clean_html_text(text: str) -> str:
    if not text:
        return ""
    text = html.unescape(text)
    text = re.sub(r'<[^>]*>', '', text)
    return text.replace('"', "'")


def publisher_from_url(link: str) -> str:
    if "naver.com" in link:
        m = re.search(r'article/(\d+)/', link)
        if m:
            oid = m.group(1).zfill(3)
            if oid in OID_MAP:
                return OID_MAP[oid]
    try:
        domain = link.split('//')[-1].split('/')[0].lower()
        domain = re.sub(r'^(www\.|n\.|news\.|m\.|blog\.|sports\.)', '', domain)
        for key, name in FIXED_MAP.items():
            if key in domain:
                return name
        return domain.split('.')[0].upper()
    except Exception:
        return "ê¸°íƒ€ë§¤ì²´"



def fetch_naver_article_info(link: str) -> dict:
    result = {"publisher": publisher_from_url(link), "pick": ""}
    if "naver.com" not in link:
        return result
    try:
        res = requests.get(link, headers=HEADERS, timeout=REQUEST_TIMEOUT)
        if res.status_code != 200:
            return result
        soup = BeautifulSoup(res.text, 'html.parser')

        publisher = ""
        logo = soup.select_one('a.press_logo img, .media_end_head_top a img')
        if logo:
            publisher = logo.get('alt', '').strip()
        if not publisher:
            meta = soup.find('meta', property='og:article:author')
            if meta:
                publisher = meta.get('content', '').strip()
        if not publisher:
            press_tag = soup.select_one('.media_end_linked_more_point')
            if press_tag:
                publisher = press_tag.get_text(strip=True)
        if publisher:
            result["publisher"] = publisher

        # â”€â”€ PICK ì—¬ë¶€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if soup.select_one('.is_pick, .media_end_head_journalist_edit_label'):
            result["pick"] = "PICK"
        elif "PICK" in res.text:
            result["pick"] = "PICK"



    except Exception:
        pass
    return result


def build_excel(df: pd.DataFrame) -> bytes:
    """DataFrame â†’ ì„œì‹ ì ìš© ì—‘ì…€ ë°”ì´íŠ¸ ë°˜í™˜"""
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df.to_excel(writer, index=False, sheet_name='ë‰´ìŠ¤í´ë¦¬í•‘')
        workbook  = writer.book
        worksheet = writer.sheets['ë‰´ìŠ¤í´ë¦¬í•‘']

        header_fmt = workbook.add_format({
            'bold': True, 'bg_color': '#2C3E50', 'font_color': '#FFFFFF',
            'border': 1, 'align': 'center', 'valign': 'vcenter',
        })
        for col_num, col_name in enumerate(df.columns):
            worksheet.write(0, col_num, col_name, header_fmt)

        col_widths = {"ê·¸ë£¹": 8, "ë§¤ì²´ëª…": 16, "ì œëª©": 60, "PICK": 6, "ê²Œì‹œì¼": 18}
        for col_num, col_name in enumerate(df.columns):
            worksheet.set_column(col_num, col_num, col_widths.get(col_name, 12))

        border_fmt_cache = {}
        for row_num, row in df.iterrows():
            group = row["ê·¸ë£¹"]
            color = GROUP_COLORS.get(group, "#FFFFFF")
            if color not in border_fmt_cache:
                border_fmt_cache[color] = workbook.add_format({
                    'bg_color': color, 'border': 1, 'valign': 'vcenter',
                })
            cell_fmt = border_fmt_cache[color]
            excel_row = row_num + 1
            for col_num, col_name in enumerate(df.columns):
                value = row[col_name]
                if col_name == "ì œëª©":
                    worksheet.write_formula(excel_row, col_num, value, cell_fmt)
                else:
                    worksheet.write(excel_row, col_num, value, cell_fmt)

        worksheet.freeze_panes(1, 0)
        worksheet.autofilter(0, 0, len(df), len(df.columns) - 1)

    return output.getvalue()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  í•µì‹¬ ìˆ˜ì§‘ ë¡œì§ (Streamlit progress barì™€ ì—°ë™)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def run_search(query: str, client_id: str, client_secret: str,
               progress_bar, status_text, days: int = 7) -> pd.DataFrame | None:

    naver_headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
    }
    kst = timezone(timedelta(hours=9))
    now = datetime.now(kst)
    since = now - timedelta(days=days)

    # â”€â”€ Step 1: API ìˆ˜ì§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    raw_items = []
    status_text.text(f"ğŸ” '{query}' ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘...")
    progress_bar.progress(5)

    for start_index in [1, 101]:
        url = (
            f"https://openapi.naver.com/v1/search/news.json"
            f"?query={query}&display=100&start={start_index}&sort=date"
        )
        try:
            res = requests.get(url, headers=naver_headers, timeout=10)
            if res.status_code != 200:
                st.error(f"ë„¤ì´ë²„ API ì˜¤ë¥˜: {res.status_code} â€” API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                return None

            items = res.json().get('items', [])
            if not items:
                break

            stop_early = False
            for item in items:
                pub_date = datetime.strptime(
                    item['pubDate'], '%a, %d %b %Y %H:%M:%S +0900'
                ).replace(tzinfo=kst)
                if pub_date < since:
                    stop_early = True
                    break
                raw_items.append({
                    "pub_date": pub_date,
                    "link": item.get('link', ''),
                    "title": clean_html_text(item.get('title', '')),
                })
            if stop_early:
                break
            time.sleep(0.2)

        except Exception as e:
            st.error(f"API ìš”ì²­ ì˜¤ë¥˜: {e}")
            return None

    if not raw_items:
        st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    status_text.text(f"ğŸ“° {len(raw_items)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ â€” ë§¤ì²´ëª… Â· PICK í¬ë¡¤ë§ ì¤‘...")
    progress_bar.progress(20)

    # â”€â”€ Step 2: ë³‘ë ¬ í¬ë¡¤ë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    crawl_results = {}
    total = len(raw_items)

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_idx = {
            executor.submit(fetch_naver_article_info, item["link"]): idx
            for idx, item in enumerate(raw_items)
        }
        done = 0
        for future in as_completed(future_to_idx):
            idx = future_to_idx[future]
            try:
                crawl_results[idx] = future.result()
            except Exception:
                crawl_results[idx] = {
                    "publisher": publisher_from_url(raw_items[idx]["link"]),
                    "pick": ""
                }
            done += 1
            pct = 20 + int(done / total * 70)   # 20~90% êµ¬ê°„
            progress_bar.progress(pct)
            status_text.text(f"ğŸ”„ í¬ë¡¤ë§ ì§„í–‰: {done} / {total}")

    # â”€â”€ Step 3: DataFrame êµ¬ì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    status_text.text("ğŸ“Š ë°ì´í„° ì •ë¦¬ ì¤‘...")
    progress_bar.progress(95)

    news_data = []
    for idx, item in enumerate(raw_items):
        info      = crawl_results.get(idx, {})
        publisher = info.get("publisher", "ê¸°íƒ€ë§¤ì²´")
        pick_val  = info.get("pick", "")
        group_val = GROUP_MAP.get(publisher, "")
        link      = item["link"]
        title     = item["title"].replace('"', "'")
        news_data.append({
            "ê·¸ë£¹":   group_val,
            "ë§¤ì²´ëª…": publisher,
            "ì œëª©":   f'=HYPERLINK("{link}", "{title}")',
            "ì œëª©_í‘œì‹œ": title,   # í™”ë©´ í‘œì‹œìš© (ìˆ˜ì‹ ì—†ëŠ” ë²„ì „)
            "ë§í¬":   link,
            "PICK":   pick_val,
            "ê²Œì‹œì¼": item["pub_date"].strftime('%Y-%m-%d %H:%M'),
        })

    progress_bar.progress(100)
    status_text.text("âœ… ì™„ë£Œ!")
    return pd.DataFrame(news_data)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Streamlit UI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.set_page_config(
    page_title="ë„¤ì´ë²„ ë‰´ìŠ¤ í´ë¦¬í•‘",
    page_icon="ğŸ“°",
    layout="wide",
)

st.title("ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ í´ë¦¬í•‘")
st.caption("í‚¤ì›Œë“œë¡œ ì›í•˜ëŠ” ê¸°ê°„ì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ê³  ì—‘ì…€ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.")

# â”€â”€ API í‚¤: st.secrets ìš°ì„  â†’ ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ í´ë°± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Streamlit Cloud: ì•± Settings > Secrets ì— ì•„ë˜ ë‚´ìš© ì¶”ê°€
#   [naver]
#   client_id     = "YOUR_CLIENT_ID"
#   client_secret = "YOUR_CLIENT_SECRET"
try:
    client_id     = st.secrets["naver"]["client_id"]
    client_secret = st.secrets["naver"]["client_secret"]
except Exception:
    import os
    client_id     = os.environ.get("NAVER_CLIENT_ID", "")
    client_secret = os.environ.get("NAVER_CLIENT_SECRET", "")

with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    if client_id and client_secret:
        st.success("âœ… API í‚¤ ì—°ê²°ë¨", icon="ğŸ”‘")
    else:
        st.error("âŒ API í‚¤ ì—†ìŒ\n`Secrets` ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
    st.divider()
    st.markdown("**ê·¸ë£¹ ìƒ‰ìƒ ê¸°ì¤€**")
    st.markdown(
        "<span style='background:#D5F5E3;padding:2px 8px;border-radius:4px;'>ê·¸ë£¹ A</span> &nbsp;"
        "<span style='background:#FEF9E7;padding:2px 8px;border-radius:4px;'>ê·¸ë£¹ B</span> &nbsp;"
        "<span style='background:#FDEBD0;padding:2px 8px;border-radius:4px;'>ê·¸ë£¹ C</span>",
        unsafe_allow_html=True
    )
    st.caption("ë¯¸ë¶„ë¥˜ ë§¤ì²´ëŠ” í°ìƒ‰ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.")
    st.divider()
    st.markdown("**ìˆ˜ì§‘ ê¸°ê°„**")
    days = st.slider("ìµœê·¼ ë©°ì¹  ê¸°ì‚¬", min_value=1, max_value=7, value=7, step=1,
                     format="%dì¼")

# â”€â”€ ë©”ì¸: ê²€ìƒ‰ ì…ë ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
col_input, col_btn = st.columns([4, 1])
with col_input:
    query = st.text_input(
        "ê²€ìƒ‰ì–´",
        placeholder="ì˜ˆ: íŒ¨ì…˜ íŠ¸ë Œë“œ",
        label_visibility="collapsed"
    )
with col_btn:
    search_clicked = st.button("ğŸ” ê²€ìƒ‰", use_container_width=True, type="primary")

# â”€â”€ ê²€ìƒ‰ ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if search_clicked:
    if not query.strip():
        st.warning("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    elif not client_id or not client_secret:
        st.error("API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Secretsë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    else:
        progress_bar = st.progress(0)
        status_text  = st.empty()

        df = run_search(query.strip(), client_id, client_secret,
                        progress_bar, status_text, days)

        if df is not None and not df.empty:
            # ì„¸ì…˜ì— ì €ì¥ (ê·¸ë£¹ í•„í„°ë§ ë“± í›„ì† ì¡°ì‘ì„ ìœ„í•´)
            st.session_state["df"]    = df
            st.session_state["query"] = query.strip()
            st.session_state["days"]  = days

# â”€â”€ ê²°ê³¼ í‘œì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "df" in st.session_state:
    df    = st.session_state["df"]
    query = st.session_state["query"]

    kst = timezone(timedelta(hours=9))
    now = datetime.now(kst)

    st.divider()

    # ìš”ì•½ ì§€í‘œ
    total   = len(df)
    cnt_a   = (df["ê·¸ë£¹"] == "ê·¸ë£¹ A").sum()
    cnt_b   = (df["ê·¸ë£¹"] == "ê·¸ë£¹ B").sum()
    cnt_c   = (df["ê·¸ë£¹"] == "ê·¸ë£¹ C").sum()
    cnt_etc = (df["ê·¸ë£¹"] == "").sum()
    cnt_pick = (df["PICK"] == "PICK").sum()

    m1, m2, m3, m4, m5, m6 = st.columns(6)
    m1.metric("ì „ì²´", f"{total}ê±´")
    m2.metric("ê·¸ë£¹ A", f"{cnt_a}ê±´")
    m3.metric("ê·¸ë£¹ B", f"{cnt_b}ê±´")
    m4.metric("ê·¸ë£¹ C", f"{cnt_c}ê±´")
    m5.metric("ë¯¸ë¶„ë¥˜", f"{cnt_etc}ê±´")
    m6.metric("PICK", f"{cnt_pick}ê±´")

    st.divider()

    # í•„í„° ì»¨íŠ¸ë¡¤
    filter_col1, filter_col2, filter_col3 = st.columns([2, 2, 2])
    with filter_col1:
        group_filter = st.multiselect(
            "ê·¸ë£¹ í•„í„°",
            options=["ê·¸ë£¹ A", "ê·¸ë£¹ B", "ê·¸ë£¹ C", "ë¯¸ë¶„ë¥˜"],
            default=["ê·¸ë£¹ A", "ê·¸ë£¹ B", "ê·¸ë£¹ C", "ë¯¸ë¶„ë¥˜"],
        )
    with filter_col2:
        pick_filter = st.checkbox("PICK ê¸°ì‚¬ë§Œ ë³´ê¸°", value=False)
    with filter_col3:
        keyword_filter = st.text_input("ì œëª© í‚¤ì›Œë“œ í•„í„°", placeholder="ì¶”ê°€ í•„í„°...")

    # í•„í„° ì ìš©
    mask = pd.Series([True] * len(df), index=df.index)

    # ê·¸ë£¹ í•„í„° (ë¯¸ë¶„ë¥˜ = "")
    selected_groups = [("" if g == "ë¯¸ë¶„ë¥˜" else g) for g in group_filter]
    mask &= df["ê·¸ë£¹"].isin(selected_groups)

    if pick_filter:
        mask &= df["PICK"] == "PICK"
    if keyword_filter.strip():
        mask &= df["ì œëª©_í‘œì‹œ"].str.contains(keyword_filter.strip(), case=False, na=False)

    df_filtered = df[mask].reset_index(drop=True)
    st.caption(f"í•„í„° ê²°ê³¼: {len(df_filtered)}ê±´")

    # â”€â”€ í…Œì´ë¸” ë Œë”ë§ (HTML) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def render_table(df_view: pd.DataFrame) -> str:
        rows_html = ""
        for _, row in df_view.iterrows():
            group     = row["ê·¸ë£¹"]
            badge_style = GROUP_BADGE.get(group, GROUP_BADGE[""])
            badge     = f'<span style="{badge_style}">{group if group else "ë¯¸ë¶„ë¥˜"}</span>'
            pick_html  = '<span style="color:#e74c3c;font-weight:bold;">PICK</span>' if row["PICK"] == "PICK" else ""
            title_html = f'<a href="{row["ë§í¬"]}" target="_blank" style="text-decoration:none;color:#1a73e8;">{row["ì œëª©_í‘œì‹œ"]}</a>'
            row_bg = GROUP_COLORS.get(group, "#FFFFFF")
            rows_html += f"""
            <tr style="background:{row_bg};">
                <td style="padding:6px 10px;border-bottom:1px solid #eee;white-space:nowrap;">{badge}</td>
                <td style="padding:6px 10px;border-bottom:1px solid #eee;white-space:nowrap;font-weight:500;">{row["ë§¤ì²´ëª…"]}</td>
                <td style="padding:6px 10px;border-bottom:1px solid #eee;">{title_html}</td>
                <td style="padding:6px 10px;border-bottom:1px solid #eee;text-align:center;">{pick_html}</td>
                <td style="padding:6px 10px;border-bottom:1px solid #eee;white-space:nowrap;color:#666;font-size:0.85em;">{row["ê²Œì‹œì¼"]}</td>
            </tr>"""

        return f"""
        <style>
            .clip-table {{ width:100%; border-collapse:collapse; font-size:0.9rem; }}
            .clip-table th {{ background:#2C3E50; color:#fff; padding:8px 10px;
                              text-align:left; position:sticky; top:0; }}
            .clip-table tr:hover {{ filter: brightness(0.96); }}
        </style>
        <div style="overflow-x:auto; max-height:600px; overflow-y:auto;">
        <table class="clip-table">
            <thead>
                <tr>
                    <th>ê·¸ë£¹</th><th>ë§¤ì²´ëª…</th><th>ì œëª©</th><th>PICK</th><th>ê²Œì‹œì¼</th>
                </tr>
            </thead>
            <tbody>{rows_html}</tbody>
        </table>
        </div>"""

    st.markdown(render_table(df_filtered), unsafe_allow_html=True)

    # â”€â”€ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.divider()
    # ì—‘ì…€ìš© df (ì œëª©_í‘œì‹œ, ë§í¬ ì»¬ëŸ¼ ì œê±°, ì œëª©ì€ HYPERLINK ìˆ˜ì‹ ìœ ì§€)
    df_excel = df_filtered[["ê·¸ë£¹", "ë§¤ì²´ëª…", "ì œëª©", "PICK", "ê²Œì‹œì¼"]].reset_index(drop=True)
    excel_bytes = build_excel(df_excel)
    file_name   = f"naver_news_{query}_{now.strftime('%Y%m%d_%H%M%S')}.xlsx"

    st.download_button(
        label="ğŸ“¥ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
        data=excel_bytes,
        file_name=file_name,
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        use_container_width=True,
        type="primary",
    )
