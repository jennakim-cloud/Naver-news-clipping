import streamlit as st
import requests
import pandas as pd
import re
import html
import time
import io
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from bs4 import BeautifulSoup

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(page_title="ë„¤ì´ë²„ ë‰´ìŠ¤ ëª¨ë‹ˆí„°ë§", layout="wide")

# --- ì„¤ì •ê°’ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€) ---
CLIENT_ID = '_xwUpsu3wHgwgduYYY3H'
CLIENT_SECRET = 'zx1KJ7Gm1o'
MAX_WORKERS = 10
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36'
}

# ê·¸ë£¹ ì»¬ëŸ¬ ë° ë§µí•‘ ë°ì´í„° (ì‚¬ìš©ì ì œê³µ ë°ì´í„°ì™€ ë™ì¼)
GROUP_COLORS = {"ê·¸ë£¹ A": "#D5F5E3", "ê·¸ë£¹ B": "#FEF9E7", "ê·¸ë£¹ C": "#FDEBD0", "": "#FFFFFF"}
# ... (FIXED_MAP, OID_MAP, GROUP_MAPì€ ì§€ë©´ìƒ ìƒëµí•˜ì§€ë§Œ ì‹¤ì œ ì½”ë“œì—ëŠ” ê·¸ëŒ€ë¡œ í¬í•¨í•˜ì„¸ìš”)
# [ì°¸ê³ : ì œê³µí•´ì£¼ì‹  FIXED_MAP, OID_MAP, GROUP_MAP ë°ì´í„°ë¥¼ ì—¬ê¸°ì— ê·¸ëŒ€ë¡œ ë¶™ì—¬ë„£ìœ¼ì‹œë©´ ë©ë‹ˆë‹¤.]

# --- í—¬í¼ í•¨ìˆ˜ (ê¸°ì¡´ ë¡œì§ ìœ ì§€) ---
def clean_html(text):
    if not text: return ""
    text = html.unescape(text)
    text = re.sub(r'<[^>]*>', '', text)
    return text.replace('"', "'")

def publisher_from_url(link):
    if "naver.com" in link:
        m = re.search(r'article/(\d+)/', link)
        if m:
            oid = m.group(1).zfill(3)
            # OID_MAP ì°¸ì¡° (ìƒëµë¨)
            return "ë„¤ì´ë²„ë‰´ìŠ¤" 
    return "ê¸°íƒ€ë§¤ì²´"

def fetch_naver_article_info(link):
    result = {"publisher": "ê¸°íƒ€ë§¤ì²´", "pick": ""}
    if "naver.com" not in link: return result
    try:
        res = requests.get(link, headers=HEADERS, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        # ë§¤ì²´ëª… ë° PICK ì¶”ì¶œ ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼)
        # ...
        result["pick"] = "PICK" if soup.select_one('.is_pick') else ""
    except: pass
    return result

# --- ë©”ì¸ UI êµ¬ì„± ---
st.title("ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ í´ë¦¬í•‘ ë„êµ¬")
st.markdown("ìµœê·¼ 7ì¼ê°„ì˜ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  ì—‘ì…€ë¡œ ì €ì¥í•©ë‹ˆë‹¤.")

with st.sidebar:
    st.header("ì„¤ì •")
    query = st.text_input("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ: ë¬´ì‹ ì‚¬")
    search_button = st.button("ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")

if search_button and query:
    kst = timezone(timedelta(hours=9))
    now = datetime.now(kst)
    seven_days_ago = now - timedelta(days=7)
    
    raw_items = []
    progress_bar = st.progress(0)
    status_text = st.empty()

    # 1. ë„¤ì´ë²„ API ìˆ˜ì§‘
    status_text.text("ğŸ” ë„¤ì´ë²„ APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
    naver_headers = {"X-Naver-Client-Id": CLIENT_ID, "X-Naver-Client-Secret": CLIENT_SECRET}
    
    for start_index in [1, 101]:
        url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display=100&start={start_index}&sort=date"
        res = requests.get(url, headers=naver_headers)
        if res.status_code == 200:
            items = res.json().get('items', [])
            for item in items:
                pub_date = datetime.strptime(item['pubDate'], '%a, %d %b %Y %H:%M:%S +0900').replace(tzinfo=kst)
                if pub_date < seven_days_ago: break
                raw_items.append({"pub_date": pub_date, "link": item['link'], "title": clean_html(item['title'])})
    
    if not raw_items:
        st.warning("ìˆ˜ì§‘ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        status_text.text(f"ğŸ”„ {len(raw_items)}ê°œ ê¸°ì‚¬ì˜ ì„¸ë¶€ ì •ë³´ë¥¼ í™•ì¸ ì¤‘ (ë³‘ë ¬ í¬ë¡¤ë§)...")
        
        # 2. ë³‘ë ¬ í¬ë¡¤ë§
        crawl_results = {}
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_idx = {executor.submit(fetch_naver_article_info, item["link"]): i for i, item in enumerate(raw_items)}
            for i, future in enumerate(as_completed(future_to_idx)):
                idx = future_to_idx[future]
                crawl_results[idx] = future.result()
                progress_bar.progress((i + 1) / len(raw_items))

        # 3. ë°ì´í„°í”„ë ˆì„ ìƒì„±
        news_data = []
        for idx, item in enumerate(raw_items):
            info = crawl_results.get(idx, {"publisher": "ê¸°íƒ€", "pick": ""})
            news_data.append({
                "ê·¸ë£¹": "ë¯¸ë¶„ë¥˜", # GROUP_MAP ì ìš© ë¡œì§ ì¶”ê°€
                "ë§¤ì²´ëª…": info['publisher'],
                "ì œëª©": item['title'],
                "URL": item['link'],
                "PICK": info['pick'],
                "ê²Œì‹œì¼": item['pub_date'].strftime('%Y-%m-%d %H:%M')
            })
        
        df = pd.DataFrame(news_data)
        
        # 4. ê²°ê³¼ ì¶œë ¥
        st.success(f"ì´ {len(df)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
        st.dataframe(df, use_container_width=True)

        # 5. ì—‘ì…€ íŒŒì¼ ìƒì„± (ë©”ëª¨ë¦¬ ë²„í¼ í™œìš©)
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            df.to_excel(writer, index=False, sheet_name='ë‰´ìŠ¤í´ë¦¬í•‘')
            # [ê¸°ì¡´ ì„œì‹ ì ìš© ì½”ë“œë¥¼ ì—¬ê¸°ì— í¬í•¨]
            writer.close()
        
        st.download_button(
            label="ì—‘ì…€ íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
            data=output.getvalue(),
            file_name=f"news_report_{query}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )